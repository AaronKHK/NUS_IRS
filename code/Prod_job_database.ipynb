{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import warnings\n",
    "import glob, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yesplum/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.feature_extraction.stop_words module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.feature_extraction.text. Anything that cannot be imported from sklearn.feature_extraction.text is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/yesplum/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.formula.api as sm\n",
    "import patsy\n",
    "import itertools\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_curve, roc_auc_score, auc\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, KFold, GridSearchCV,learning_curve\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer \n",
    "from sklearn.feature_extraction import stop_words\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from string import printable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Header\n",
    "\n",
    "src_path = '../data_src/' #web-crawled data source path\n",
    "extension = 'csv' #data source format\n",
    "output_path = '../output/' #database output path\n",
    "\n",
    "#define salary range\n",
    "low = 3000\n",
    "med = 4500\n",
    "high = 6000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define list of filters\n",
    "\n",
    "title_key = ['DATA', 'MACHINE','ANALYST','MACHINE LEARNING','ANALYTICS', \"SCIENCE\", '4.0','APPLICATION', 'DEEP LEARNING',\n",
    "             'RESEARCH','NLP', 'ARTIFICIAL', \"INTELLIGENT\", 'AI', 'SCIENTIST','SYSTEM','Industry', 'IOT', 'FINANCE', 'FINTECH', \n",
    "             'SOFTWARE', 'ENGINEER', 'ENGINEERING','PROFESSOR','BUSINESS', 'DEVELOPER', 'INDUSTRIAL','AUTOMATION', 'CLOUD',\n",
    "             'SOLUTION','ARCHITECT', 'MANAGER','VP','PRESIDENT', 'TECHNOLOGY', 'SPECIALIST', 'TECHNICAL','LEAD','TECHNOLOGIST']\n",
    "\n",
    "unwanted_title_key = ['PHYSIOTHERAPIST','ACCOUNT','AUDIT','COUNSEL','EXECUTIVE','SALES','GENERAL','MARKET','ELECTRICAL','BUSINESS',\n",
    "                      'ADMIN','CUSTOMER','OFFICER','OPERATION', 'MECHANICAL','CHEMICAL','COORDINATOR','LECTURER','TECHNICIAN']\n",
    "\n",
    "unwanted_cat_key = ['HUMAN','SOCIAL','THERAPY','TAXATION','CUSTOMER','INTERIOR', 'ADMIN','BUILDING', 'SECRETARIAL','INVESTIGATION', \n",
    "                'AUDITING', 'ENVIRONMENT','SALES', 'MARKETING','ADVERTISING','CONSTRUCTION', 'DESIGN','LEGAL','HOSPITALITY',\n",
    "                'PROFESSIONAL']\n",
    "\n",
    "cat_list = ['Information Technology', 'Telecommunications', 'Engineering','Sciences', 'Finance','Healthcare','Management',\n",
    "            'Consulting','Logistics', 'Civil', 'Others']\n",
    "\n",
    "edu_list = ['phd','doctor','master','degree','computer science','engineering','statistic','math','computer engineering',\n",
    "            'business','ph.d']\n",
    "\n",
    "skills_list = ['python','java','scala','hadoop','sql','spark','tensorflow','scikit','linux','pytorch','theano','caffe','Matlab',\n",
    "               'perl','deep learning','nlp','apache','mapreduce','aws','azure','container','kafka','cassandra', 'c\\++' ,'julia',\n",
    "               'jupyter','nltk','tableau','power bi','sas','pandas','git','hive','impala','agile','machine learning','bash',\n",
    "               'natural language','oracle','cloud','flask','golang','optimization','c#','opencv','computer vision','api','jira',\n",
    "               'unix','bash','docker','keras', 'qlik','gcp','scrum', 'airflow','.net','d3.js']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_ingestion(src_path, extension, output_path):\n",
    "    \n",
    "    all_filenames = [i for i in glob.glob('{}*.{}'.format(src_path,extension))] \n",
    "    print('Source file(s) loaded:', all_filenames)\n",
    "    \n",
    "    #combine all files in the list \n",
    "\n",
    "    df_raw = pd.concat([pd.read_csv(f, encoding='unicode escape',skiprows=0) for f in all_filenames ]) \n",
    "    df_raw.reset_index(inplace=True) \n",
    "    df_raw = df_raw.drop(columns=['index','Unnamed: 0']) \n",
    "    \n",
    "    return df_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_clean(df,title_key,drop_title_key,drop_cat_key):\n",
    "    \n",
    "    #remove duplicate based on Job ID\n",
    "    job_clean = df_raw.drop_duplicates(subset='Job_Id', keep='first')\n",
    "    #drop Salary_Type due to only one unique value 'Monthly'\n",
    "    job_clean = job_clean.drop(columns='Salary_Type')\n",
    "    #remove job without title\n",
    "    job_no_title = job_clean['Job_Title'] == ''\n",
    "    job_clean = job_clean[~job_no_title]\n",
    "    #remove row with all NaN value\n",
    "    job_clean[job_clean.isnull().any(axis=1)]\n",
    "    job_clean = job_clean.dropna()\n",
    "    \n",
    "    #perform data cleaning on every row and columms\n",
    "    clean_list = \"(\\[|\\]|b'|Requirements|'|amp;|xa0|\\\\\\|xe2x80x93|\\\\n|div class=|div class=|span class=|dib|lh-solid|/span|f5-5 i fw4 gray|f5 fw4 ph1|<|>|/div|\\\")\"\n",
    "    for col in job_clean.columns.difference(['Requirements']):\n",
    "        job_clean[col]=job_clean[col].str.replace(clean_list, \"\")\n",
    "\n",
    "    #space remain for Requirements column    \n",
    "    job_clean['Requirements']=job_clean['Requirements'].str.replace(clean_list, \" \")\n",
    "\n",
    "    #remove all non-ascii char except punctuation, digits, ascii_letters and whitespace\n",
    "    job_clean['Requirements'] = job_clean['Requirements'].apply(lambda y: ''.join(filter(lambda x: x in printable, y)))\n",
    "    \n",
    "    #further remove job with same data from all columns\n",
    "    job_clean = job_clean.drop_duplicates(subset=job_clean.columns, keep='first') \n",
    "        \n",
    "    #further filter on job title with specific keywords\n",
    "\n",
    "    key = '|'.join(title_key)\n",
    "    data_job = job_clean['Job_Title'].str.upper().str.contains(key)\n",
    "    job_clean = job_clean[data_job]\n",
    "\n",
    "    #remove job title with unwanted keywords\n",
    "    \n",
    "    key2 = '|'.join(drop_title_key)\n",
    "    non_data_job = job_clean['Job_Title'].str.upper().str.contains(key2)\n",
    "    job_clean = job_clean[~non_data_job]\n",
    "    \n",
    "    #remove job with multiple category\n",
    "    cat_list = \"(/|and)\"\n",
    "    job_clean['Category']=job_clean['Category'].str.replace(cat_list, \",\")\n",
    "    job_clean['Cat_num'] = job_clean['Category'].str.count(',')\n",
    "    \n",
    "    multiple_cat = job_clean['Cat_num']>5\n",
    "    job_clean = job_clean[~multiple_cat]\n",
    "    job_clean = job_clean.drop(columns='Cat_num')\n",
    "    \n",
    "    #remove job with no or multiple seniority\n",
    "    senior_rule = (job_clean['Seniority'].str.count(',')>=1) | (job_clean['Seniority']=='')\n",
    "    job_clean = job_clean[~senior_rule]\n",
    "   \n",
    "    #remove job cat with specific keywords\n",
    "\n",
    "    key3 = '|'.join(drop_cat_key)\n",
    "    rare_cat = job_clean['Category'].str.upper().str.contains(key3)\n",
    "    job_clean = job_clean[~rare_cat]\n",
    "    \n",
    "    #remove row without salary\n",
    "    no_salary = job_clean['Salary_Range'].str.contains('Salary undisclosed')\n",
    "    df_salary = job_clean[~no_salary]\n",
    "    df_no_salary = job_clean[no_salary]\n",
    "    df_salary = df_salary.reset_index(drop=True)\n",
    "    \n",
    "    req_empty = []\n",
    "\n",
    "    for i in range (len(df_salary)):\n",
    "    \n",
    "        if((len(df_salary['Requirements'][i]))<5):\n",
    "            req_empty.append(i)\n",
    "           \n",
    "    #clean & remove row without requirements\n",
    "    df_salary['Requirements']=df_salary['Requirements'].str.replace('(\\n)', \"\")\n",
    "    df_salary = df_salary.drop(req_empty)\n",
    "    df_salary = df_salary.reset_index(drop=True)\n",
    "\n",
    "    return df_salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source file(s) loaded: ['../data_src/DS_27Mar2020a.csv', '../data_src/DS_27Mar2020b.csv', '../data_src/ML_27Mar2020b.csv', '../data_src/ML_27Mar2020a.csv', '../data_src/AI_26Mar2020.csv']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(463, 10)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw = data_ingestion(src_path, extension, output_path)\n",
    "df = data_clean(df_raw, title_key, unwanted_title_key, unwanted_cat_key)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def salary_feature(df, low, med, high):\n",
    "    \n",
    "    #extract salary columns due to contain multiple information\n",
    "    salary_range = df[\"Salary_Range\"].str.split(\"to\", n = 2, expand = True) \n",
    "\n",
    "    #Give columns name to the dataframe\n",
    "    salary_range = salary_range.rename({0:'Min_Salary',1:'Max_Salary'}, axis='columns')\n",
    "\n",
    "    #removed $ and , from salary \n",
    "    for col in salary_range.columns:\n",
    "        salary_range[col]=salary_range[col].str.replace('(\\$|,)', '')\n",
    "\n",
    "    #convert from ojbect to float for statistical infomation\n",
    "    salary_range['Min_Salary'] = salary_range['Min_Salary'].astype('float64')\n",
    "    salary_range['Max_Salary'] = salary_range['Max_Salary'].astype('float64')\n",
    "    \n",
    "    #concat min_max salary dataframe with salary range dataframe\n",
    "    df_salary1 = pd.concat([df, salary_range], axis=1)\n",
    "    df_salary1 = df_salary1.drop(columns='Salary_Range')  \n",
    "    \n",
    "    #create a condition to check for high outliers\n",
    "    abovemean_min = round(10*np.mean(df_salary1['Min_Salary']),0)\n",
    "    abovemean_max = round(10*np.mean(df_salary1['Max_Salary']),0)\n",
    "    \n",
    "    #convert yearly salary into monthly salary\n",
    "\n",
    "    df_salary1['Min_Salary'] = np.where((df_salary1['Min_Salary'] > abovemean_min),\n",
    "                                    round((df_salary1['Min_Salary']/12),0), df_salary1['Min_Salary'])\n",
    "\n",
    "    df_salary1['Max_Salary'] = np.where((df_salary1['Max_Salary'] > abovemean_min),\n",
    "                                    round((df_salary1['Max_Salary']/12),0), df_salary1['Max_Salary'])\n",
    "    \n",
    "    #drop unrealistic min and max monthly salary range (which is more than 10 times)\n",
    "    min_max_abnormal = (df_salary1['Max_Salary']>10*df_salary1['Min_Salary'])\n",
    "    df_salary1 = df_salary1[~min_max_abnormal]\n",
    "    \n",
    "    #drop job with max salary less than 2500, assuming data entry/admin/operator job\n",
    "    low_sal = ((df_salary1['Min_Salary']<=1800) | (df_salary1['Max_Salary']<=2500))\n",
    "    df_salary1 = df_salary1[~low_sal]\n",
    "    \n",
    "    #create new feature for average salary\n",
    "    df_salary1['Avg_Salary'] = (df_salary1['Min_Salary'] + df_salary1['Max_Salary']) / 2\n",
    "    \n",
    "    #drop job with outlier salary\n",
    "    salary_outlier = ((df_salary1['Avg_Salary']>20000) | (df_salary1['Avg_Salary']<3000))\n",
    "    df_salary1 = df_salary1[~salary_outlier]\n",
    "    \n",
    "    #bin salary into 3 groups:\n",
    "    #3000 to 4500 - Low\n",
    "    #4500 to 6000 - Med\n",
    "    #6000 and above - High\n",
    "\n",
    "    bins = [low, med, high, np.inf]\n",
    "    names = ['Low', 'Med', 'High']\n",
    "\n",
    "    df_salary1['Salary_range'] = pd.cut(df_salary1['Avg_Salary'], bins, labels=names)\n",
    "    df_salary1 = df_salary1.reset_index(drop=True)\n",
    "    \n",
    "    return df_salary1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(462, 13)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = salary_feature(df, low, med, high)\n",
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emp_type(df):\n",
    "    \n",
    "    #remove others employment type\n",
    "    type_key = ['PART TIME','TEMPORARY','INTERNSHIP','FLEXI','FREELANCE']\n",
    "    key = '|'.join(type_key)\n",
    "    non_type = df['Emp_Type'].str.upper().str.contains(key)\n",
    "    df = df[~non_type]\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    #consolidate employment type\n",
    "    consolidate = \"(Full Time|Permanent, Full Time)\"\n",
    "    df['Emp_Type']=df['Emp_Type'].str.replace(consolidate, \"Permanent\")\n",
    "\n",
    "    consolidate = \"(Contract, Full Time)\"\n",
    "    df['Emp_Type']=df['Emp_Type'].str.replace(consolidate, \"Contract\")\n",
    "\n",
    "    consolidate = \"(Contract, Permanent, Full Time)\"\n",
    "    df['Emp_Type']=df['Emp_Type'].str.replace(consolidate, \"Cont_Perm\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_name(df):\n",
    "    \n",
    "    stacked = pd.DataFrame(df['Category'].str.split(',').tolist()).stack()\n",
    "    cat_count = pd.DataFrame(stacked.value_counts(), columns=['Count']).reset_index()\n",
    "    cat_count1 = []\n",
    "\n",
    "    for i in range (len(cat_count)):\n",
    "        cat_count1.append(cat_count['index'][i].lstrip())\n",
    "    \n",
    "    cat_name = list(dict.fromkeys(cat_count1))\n",
    "    \n",
    "    return cat_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_kie(df):\n",
    "\n",
    "    #extract only number from string\n",
    "    df['Year_Experience'] = df['Year_Experience'].str.extract('(\\d+)')\n",
    "    \n",
    "    #remove comma from cell with string\n",
    "    clean_list = \"(,|;|â||¦|®|)\"\n",
    "    for col in df.columns.difference(['Year_Experience','Min_Salary','Max_Salary','Avg_Salary']):\n",
    "        df[col]=df[col].str.replace(clean_list, \"\")\n",
    "        \n",
    "    #remove extra whitespace between string\n",
    "    df = df.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
    "    \n",
    "    #remove row without year of experience\n",
    "    df = df.dropna(subset=['Year_Experience']).reset_index(drop=True)\n",
    "    \n",
    "    #fill NaN in year of experience with 0\n",
    "    #df['Year_Experience'] = df['Year_Experience'].fillna(0)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = emp_type(df1)\n",
    "df1 = clean_kie(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def job_edu_cat(df, cat_list, edu_list):\n",
    "    \n",
    "    for cat in cat_list:\n",
    "        df[cat] = np.where(df['Category'].str.lower().str.contains(cat),1,0)\n",
    "        \n",
    "    for edu in edu_list:\n",
    "        df[edu] = np.where(df['Requirements'].str.lower().str.contains(edu),edu,0) #replace with 1 for machine learning\n",
    "    \n",
    "    sum_elements = [f\"df['{col}']\" for col in edu_list]\n",
    "    to_eval = \"+ '_' + \".join(sum_elements)\n",
    "    df['Qualification'] = eval(to_eval)\n",
    "    clean_list = \"(0|_0|0_|_0_)\"\n",
    "    df['Qualification']=df['Qualification'].str.replace(clean_list, \"\")\n",
    "    df['Qualification']=df['Qualification'].str.replace('_', '%')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skill_cat(df, skills_list):\n",
    "    \n",
    "    for skill in skills_list:\n",
    "        df[skill] = np.where(df['Requirements'].str.lower().str.contains(skill),skill,0) #replace skill with 1 for machine learning\n",
    "        \n",
    "    sum_elements = [f\"df['{col}']\" for col in skills_list]\n",
    "    to_eval = \"+ '_' + \".join(sum_elements)\n",
    "    df['New_Skills'] = eval(to_eval)\n",
    "    clean_list = \"(0|_0|0_|_0_)\"\n",
    "    df['New_Skills']=df['New_Skills'].str.replace(clean_list, \"\")\n",
    "    df['New_Skills']=df['New_Skills'].str.replace('_', '%')\n",
    "    df['New_Skills'] = df['New_Skills'].str.replace(\"\\\\\", \"\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = job_edu_cat(df1, cat_list, edu_list)\n",
    "df1 = skill_cat(df1, skills_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word count function\n",
    "def word_count(df_col):\n",
    "\n",
    "    str_counts = 0\n",
    "    sum_str = 0\n",
    "\n",
    "    for i in range (len(df_col)):    \n",
    "        str_counts = len(df_col[i].split())\n",
    "        sum_str = sum_str + str_counts\n",
    "\n",
    "    print(sum_str)\n",
    "    \n",
    "#frequent word function\n",
    "def freq_words(word_count, features):\n",
    "\n",
    "    num_word = np.asarray(word_count.sum(axis=0)).reshape(-1)\n",
    "    most_count = num_word.argsort()[::-1]\n",
    "    key_word = pd.Series(num_word[most_count], \n",
    "                           index=features[most_count])\n",
    "\n",
    "    return key_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_word_fil(df):\n",
    "    \n",
    "    #stop words were added to filter some generic recurring business terms.\n",
    "    stop = stopwords.words('english')\n",
    "    stop += ['regret','shortlisted', 'candidates','notified','etc', 'take', 'hands','added','able','writting',\n",
    "             'year','years','least', 'related','using', 'and', 'ability','work','skills','advantage','written'\n",
    "            'develop','good','team','design','knowledge','experience','following','areas', 'ability','and','in','to']\n",
    "    \n",
    "    #most common words for requirements\n",
    "    cvt = CountVectorizer(lowercase=True, strip_accents='unicode',max_features=80000, min_df=1, max_df=0.9,\n",
    "                          stop_words=stop, ngram_range=(1,2))\n",
    "    vect_word = cvt.fit_transform(df['Requirements'])\n",
    "    features = np.array(cvt.get_feature_names()) \n",
    "\n",
    "    key_word = freq_words(vect_word, features)\n",
    "    \n",
    "    #update stop_word with common words\n",
    "    new_stop = key_word[key_word<5].index\n",
    "    stop.extend(new_stop)\n",
    "    \n",
    "    #number of word found in Requirements column before clean\n",
    "    print('Number of words before filter:')\n",
    "    word_count(df['Requirements'])\n",
    "    \n",
    "    pat = r'\\b(?:{})\\b'.format('|'.join(stop))\n",
    "    df['Requirements'] = df['Requirements'].str.replace(pat, \" \")\n",
    "    df['Requirements'] = df['Requirements'].map(lambda x: x.strip())\n",
    "    df['Requirements'] = df['Requirements'].replace({' +':\" \"},regex=True)\n",
    "    \n",
    "    print('Number of words after filter:')\n",
    "    word_count(df['Requirements'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words before filter:\n",
      "68149\n",
      "Number of words after filter:\n",
      "33574\n"
     ]
    }
   ],
   "source": [
    "data_df = stop_word_fil(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save output file skills_list\n",
    "def save_file(df, output_path):\n",
    "\n",
    "    df = df.drop(columns=data_df[skills_list+cat_list+edu_list].columns)\n",
    "    df.to_csv('{}JOB_DATA_v15.csv'.format(output_path), index=False, encoding='utf-8')\n",
    "    print('File saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved\n"
     ]
    }
   ],
   "source": [
    "save_file(data_df, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(357, 106)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dummified Seniority columns to use as predictor features\n",
    "seniority_cat=data_df['Seniority'].str.get_dummies()\n",
    "emp_cat=data_df['Emp_Type'].str.get_dummies()\n",
    "df = pd.concat([data_df, seniority_cat, emp_cat], axis=1)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=0.6, max_features=500, min_df=10,\n",
       "                ngram_range=(1, 2), preprocessor=None,\n",
       "                stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                            'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                            \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                            'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                            'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                            'itself', ...],\n",
       "                strip_accents='unicode', token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CountVectorizer job requirements columns\n",
    "#min_df=10, max_df=0.6, max_features=500, ngram_range=(1,2), f1 = 0.61\n",
    "stop_ml = stopwords.words('english')\n",
    "\n",
    "stop_ml += ['possess','work','back','ability','communication', 'participate', 'like', 'tools','distributed',\n",
    "            'contribute','proven','engage','understanding','excellent', 'teams','experienced', 'familiarity',\n",
    "            'partners', 'study', 'well','preferably','user','field','experience','english', 'level','sets',\n",
    "            'delivery','implementation','relevant','state','exposure','record','problems','define','open',\n",
    "            'proficient','understand']\n",
    "\n",
    "cvec = CountVectorizer(lowercase=True, strip_accents='unicode',\n",
    "                       max_features=500, min_df=10, max_df=0.6, \n",
    "                       stop_words=stop_ml,ngram_range=(1,2))\n",
    "cvec.fit(df['Requirements'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating predictor and target dataset\n",
    "model_data = df.drop(columns=['Job_Title','Company','Seniority','Category','Min_Salary',\n",
    "                               'Max_Salary','Emp_Type','Avg_Salary', 'Job_Id', 'Date_Posted'])\n",
    "\n",
    "nlp = pd.DataFrame(cvec.transform(model_data['Requirements']).todense(),columns=cvec.get_feature_names())\n",
    "\n",
    "senior_nlp = pd.concat([model_data, nlp], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(357, 94)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = model_data.drop(columns=['Salary_range','Requirements'])\n",
    "X_nlp = senior_nlp.drop(columns=['Salary_range','Requirements'])\n",
    "y = senior_nlp['Salary_range'].values\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data with dummified 'seniority' and countvectorized 'requirements'\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data with countvectorized 'requirements' only\n",
    "X_train_nlp, X_test_nlp, y_train_nlp, y_test_nlp = train_test_split(X_nlp, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'phd'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-7a155a28848c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdtc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdtc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdtc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdtc_nlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_nlp\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    875\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 877\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m    878\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    529\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"unsafe\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \"\"\"\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'phd'"
     ]
    }
   ],
   "source": [
    "dtc = DecisionTreeClassifier(max_depth=4, random_state=42)\n",
    "dtc = dtc.fit(X_train , y_train)\n",
    "\n",
    "dtc1 = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "dtc_nlp = dtc1.fit(X_nlp , y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Low       0.83      0.70      0.76        69\n",
      "         Med       0.33      0.18      0.24        11\n",
      "        High       0.43      0.69      0.53        26\n",
      "\n",
      "    accuracy                           0.64       106\n",
      "   macro avg       0.53      0.52      0.51       106\n",
      "weighted avg       0.68      0.64      0.65       106\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,dtc.predict(X_test),target_names=[\"Low\", \"Med\", \"High\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pred Low</th>\n",
       "      <th>Pred Med</th>\n",
       "      <th>Pred High</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual Low</th>\n",
       "      <td>48</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual Med</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual High</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Pred Low  Pred Med  Pred High\n",
       "Actual Low         48         4         17\n",
       "Actual Med          2         2          7\n",
       "Actual High         8         0         18"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(confusion_matrix(y_test,dtc.predict(X_test)),\n",
    "             index=['Actual Low','Actual Med', 'Actual High'],\n",
    "             columns=['Pred Low','Pred Med','Pred High'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Low       0.93      0.82      0.87       226\n",
      "         Med       0.65      0.70      0.68        37\n",
      "        High       0.58      0.73      0.64        89\n",
      "\n",
      "    accuracy                           0.78       352\n",
      "   macro avg       0.72      0.75      0.73       352\n",
      "weighted avg       0.81      0.78      0.79       352\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y,dtc_nlp.predict(X_nlp),target_names=[\"Low\", \"Med\", \"High\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pred Low</th>\n",
       "      <th>Pred Med</th>\n",
       "      <th>Pred High</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual Low</th>\n",
       "      <td>185</td>\n",
       "      <td>3</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual Med</th>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual High</th>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Pred Low  Pred Med  Pred High\n",
       "Actual Low        185         3         38\n",
       "Actual Med          1        26         10\n",
       "Actual High        13        11         65"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(confusion_matrix(y,dtc_nlp.predict(X_nlp)),\n",
    "             index=['Actual Low','Actual Med', 'Actual High'],\n",
    "             columns=['Pred Low','Pred Med','Pred High'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>abs coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Year_Experience</th>\n",
       "      <td>0.405679</td>\n",
       "      <td>0.405679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sciences</th>\n",
       "      <td>0.098422</td>\n",
       "      <td>0.098422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>java scala</th>\n",
       "      <td>0.083142</td>\n",
       "      <td>0.083142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Non-executive</th>\n",
       "      <td>0.066195</td>\n",
       "      <td>0.066195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>phd</th>\n",
       "      <td>0.061634</td>\n",
       "      <td>0.061634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>python java</th>\n",
       "      <td>0.046882</td>\n",
       "      <td>0.046882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>algorithms</th>\n",
       "      <td>0.043642</td>\n",
       "      <td>0.043642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>science engineering</th>\n",
       "      <td>0.043628</td>\n",
       "      <td>0.043628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>opencv</th>\n",
       "      <td>0.037369</td>\n",
       "      <td>0.037369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>science computer</th>\n",
       "      <td>0.033332</td>\n",
       "      <td>0.033332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>software</th>\n",
       "      <td>0.030956</td>\n",
       "      <td>0.030956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Junior Executive</th>\n",
       "      <td>0.026203</td>\n",
       "      <td>0.026203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>0.018749</td>\n",
       "      <td>0.018749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>variety</th>\n",
       "      <td>0.004167</td>\n",
       "      <td>0.004167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>platforms</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         coef  abs coef\n",
       "Year_Experience      0.405679  0.405679\n",
       "Sciences             0.098422  0.098422\n",
       "java scala           0.083142  0.083142\n",
       "Non-executive        0.066195  0.066195\n",
       "phd                  0.061634  0.061634\n",
       "python java          0.046882  0.046882\n",
       "algorithms           0.043642  0.043642\n",
       "science engineering  0.043628  0.043628\n",
       "opencv               0.037369  0.037369\n",
       "science computer     0.033332  0.033332\n",
       "software             0.030956  0.030956\n",
       "Junior Executive     0.026203  0.026203\n",
       "test                 0.018749  0.018749\n",
       "variety              0.004167  0.004167\n",
       "platforms            0.000000  0.000000"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = np.array(X_nlp.columns)\n",
    "dt_coefs = pd.DataFrame({'coef':dtc_nlp.feature_importances_, 'abs coef':abs(dtc_nlp.feature_importances_)},index=features)\n",
    "dt_coefs = dt_coefs.sort_values('coef',ascending=False)\n",
    "dt_coefs.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree.export import export_text\n",
    "\n",
    "tree_rules = export_text(dtc, feature_names=list(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "#clf = tree.DecisionTreeClassifier(max_leaf_nodes=n)\n",
    "#clf_ = clf.fit(X, data_y)\n",
    "\n",
    "feature_names = X_nlp.columns\n",
    "class_name = dtc_nlp.classes_.astype(str)\n",
    "\n",
    "def output_pdf(model):\n",
    "    from sklearn import tree\n",
    "    from sklearn.externals.six import StringIO\n",
    "    import pydot_ng as pydot\n",
    "    dot_data = StringIO()\n",
    "    tree.export_graphviz(model, out_file=dot_data,\n",
    "                         feature_names=feature_names,\n",
    "                         class_names=class_name,\n",
    "                         filled=True, rounded=True,\n",
    "                         special_characters=True,\n",
    "                          node_ids=1,)\n",
    "    graph = pydot.graph_from_dot_data(dot_data.getvalue())\n",
    "    graph.write_pdf(\"{}DT.pdf\".format(output_path))\n",
    "\n",
    "output_pdf(dtc_nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
